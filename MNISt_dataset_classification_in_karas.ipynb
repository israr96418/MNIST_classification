{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8bc8e75",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee7cb9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KERAS_BACKEND=tensorflow'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.initializers import RandomNormal\n",
    "import seaborn as sns\n",
    "# if bydefault not using tensorflow as backend for keras then do this command\n",
    "\"KERAS_BACKEND=tensorflow\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d6cec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw plotting in matplot lab for much more understanding \n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as ptl\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# dynamic_plot function --->so this function automatically update the plot for each epoch and error\n",
    "def dynamic_plot(x,vy ,ty ,ax, colors=['b']):\n",
    "    ax.plot(x,vy,'b',label=\"validation_loss\")\n",
    "    ax.plot(x,ty,'r', label=\"Train_loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4586d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data in mnist dataset\n",
    "# shuffled and split b/w train and test\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "348af7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data in train dataset: 60000 and each image is shape of (28,28) pixel\n",
      "Number of data in test dataset: 10000 and each image is shape of (28,28) pixel\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data in train dataset:\", X_train.shape[0],\"and each image is shape of (%d,%d) pixel\"%(X_train.shape[1], X_train.shape[2]))\n",
    "print(\"Number of data in test dataset:\", X_test.shape[0],\"and each image is shape of (%d,%d) pixel\"%(X_test.shape[1],X_test.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5878b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you observe the input image is 2 dimensional vector\n",
    "# for each image we have a (28*28) vector\n",
    "# we will convert the (28*28) vector into a single dimensional vector(1*784)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9bb9eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data in train dataset: 60000 and each image is shape of (784) pixel\n",
      "Number of data in test dataset: 10000 and each image is shape of (784) pixel\n"
     ]
    }
   ],
   "source": [
    "# after converting the 2d vector into 1d vector\n",
    "print(\"Number of data in train dataset:\", X_train.shape[0],\"and each image is shape of (%d) pixel\"%(X_train.shape[1]))\n",
    "print(\"Number of data in test dataset:\", X_test.shape[0],\"and each image is shape of (%d) pixel\"%(X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82aea230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n",
      " 247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n",
      " 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n",
      "   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n",
      "  82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n",
      " 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n",
      " 225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      " 253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n",
      " 253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n",
      "  80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# an Example datapont\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "035adeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you observe the above matric each cell containing value from 0-255\n",
    "# before going to ML model first we normalized the data\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea5996d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215686\n",
      " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
      " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313725\n",
      " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1372549  0.94509804\n",
      " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      " 0.58823529 0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.58039216\n",
      " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058824\n",
      " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
      " 0.31372549 0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333333 0.99215686\n",
      " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# After Normalizing the data \n",
    "# An example datapoint\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2853d9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class number for first image that is store at index 0:  5\n"
     ]
    }
   ],
   "source": [
    "# here we have a class number for each image\n",
    "print(\"class number for first image that is store at index 0: \",Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "096afc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets convert this into a 10 dimensional vector\n",
    "#example: consider an image is 5 convert into 5==>[0,0,0,0,0,1,0,0,0,0]\n",
    "# this converion is very important for MLP\n",
    "Y_train = np_utils.to_categorical(Y_train,10)\n",
    "Y_test = np_utils.to_categorical(Y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad171ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After converting the output(classlabel) into a 10 dimensional vector:  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"After converting the output(classlabel) into a 10 dimensional vector: \", Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e9bab",
   "metadata": {},
   "source": [
    "# Softmax classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fb7a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Sequential model b/z data sequentially move from oinput layer to 1-hidden layer and to 2-hidden layer and soo on..\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05bfdaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some model parameter\n",
    "batch_size = 128\n",
    "output_dim = 10\n",
    "input_dim = X_train.shape[1]\n",
    "nb_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75547adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10537d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start building a mode\n",
    "model = Sequential()\n",
    "# with the add() method add layer \n",
    "\n",
    "\n",
    "# The model needs to know what input shape it should expect. \n",
    "# For this reason, the first layer in a Sequential model \n",
    "# (and only the first, because following layers can do automatic shape inference)\n",
    "# needs to receive information about its input shape. \n",
    "# you can use input_shape and input_dim to pass the shape of input\n",
    "\n",
    "# output_dim represent the number of nodes need in that layer\n",
    "# here we have 10 nodes b/z we 10 class classification\n",
    "\n",
    "\n",
    "model.add(Dense(output_dim, input_dim=input_dim, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f447f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caa8254b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 [==============================] - 3s 4ms/step - loss: 1.2849 - accuracy: 0.6935 - val_loss: 0.7969 - val_accuracy: 0.8385\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 2s 3ms/step - loss: 0.7044 - accuracy: 0.8433 - val_loss: 0.5955 - val_accuracy: 0.8672\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 2s 3ms/step - loss: 0.5774 - accuracy: 0.8618 - val_loss: 0.5158 - val_accuracy: 0.8786\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.5171 - accuracy: 0.8705 - val_loss: 0.4718 - val_accuracy: 0.8822\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.4805 - accuracy: 0.8764 - val_loss: 0.4426 - val_accuracy: 0.8873\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 2s 3ms/step - loss: 0.4554 - accuracy: 0.8812 - val_loss: 0.4222 - val_accuracy: 0.8899\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.4368 - accuracy: 0.8843 - val_loss: 0.4066 - val_accuracy: 0.8918\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 2s 3ms/step - loss: 0.4223 - accuracy: 0.8874 - val_loss: 0.3946 - val_accuracy: 0.8947\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.4106 - accuracy: 0.8897 - val_loss: 0.3844 - val_accuracy: 0.8969\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.4010 - accuracy: 0.8914 - val_loss: 0.3760 - val_accuracy: 0.8982\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.3927 - accuracy: 0.8928 - val_loss: 0.3691 - val_accuracy: 0.9003\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.3857 - accuracy: 0.8946 - val_loss: 0.3628 - val_accuracy: 0.9020\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.3794 - accuracy: 0.8958 - val_loss: 0.3578 - val_accuracy: 0.9027\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.3740 - accuracy: 0.8971 - val_loss: 0.3527 - val_accuracy: 0.9044\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.3691 - accuracy: 0.8980 - val_loss: 0.3487 - val_accuracy: 0.9051\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 2s 3ms/step - loss: 0.3647 - accuracy: 0.8991 - val_loss: 0.3447 - val_accuracy: 0.9059\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.3607 - accuracy: 0.9003 - val_loss: 0.3413 - val_accuracy: 0.9068\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 2s 3ms/step - loss: 0.3571 - accuracy: 0.9009 - val_loss: 0.3380 - val_accuracy: 0.9079\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 2s 3ms/step - loss: 0.3538 - accuracy: 0.9018 - val_loss: 0.3352 - val_accuracy: 0.9091\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 2s 3ms/step - loss: 0.3507 - accuracy: 0.9025 - val_loss: 0.3326 - val_accuracy: 0.9091\n"
     ]
    }
   ],
   "source": [
    "# Before training a model, you need to configure the learning process, which is done via the compile method\n",
    "\n",
    "# It receives three arguments:\n",
    "# An optimizer. This could be the string identifier of an existing optimizer , https://keras.io/optimizers/\n",
    "# A loss function. This is the objective that the model will try to minimize., https://keras.io/losses/\n",
    "# A list of metrics. For any classification problem you will want to set this to metrics=['accuracy'].  https://keras.io/metrics/\n",
    "\n",
    "\n",
    "# Note: when using the categorical_crossentropy loss, your targets should be in categorical format \n",
    "# (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros except \n",
    "# for a 1 at the index corresponding to the class of the sample).\n",
    "\n",
    "# that is why we converted out labels into vectors\n",
    "\n",
    "# with the help compile method we configure the learning process\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Keras models are trained on Numpy arrays of input data and labels. \n",
    "# For training a model, you will typically use the  fit function\n",
    "\n",
    "# fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, \n",
    "# validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, \n",
    "# validation_steps=None)\n",
    "\n",
    "# fit() function Trains the model for a fixed number of epochs (iterations on a dataset).\n",
    "\n",
    "# it returns A History object. Its History.history attribute is a record of training loss values and \n",
    "# metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n",
    "\n",
    "# just like sklearn--> model fit(model train)\n",
    "history = model.fit(X_train, Y_train, steps_per_epoch=500, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b32412c",
   "metadata": {},
   "source": [
    "# MLP + SigmoidActivation + SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d85f93f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 468,874\n",
      "Trainable params: 468,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# MLP architecture 512 128\n",
    "# 512 means first hidden layer having 512 Node(on each Node sigmoaid activation ftn) similarly for 128 on 2nd hidden layer\n",
    "\n",
    "# Sequential() model b/z we sequentially send the data from one layer to another\n",
    "sigmoid_model = Sequential()\n",
    "sigmoid_model.add(Dense(512, activation='sigmoid', input_shape=(input_dim,)))\n",
    "sigmoid_model.add(Dense(128, activation='sigmoid'))\n",
    "# outpu_dim represnt No of Node on softmax layer\n",
    "# here we have 10 node\n",
    "sigmoid_model.add(Dense(output_dim, activation='softmax'))\n",
    "sigmoid_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c541ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 [==============================] - 6s 10ms/step - loss: 2.1771 - accuracy: 0.4457 - val_loss: 2.1168 - val_accuracy: 0.5239\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 2.0510 - accuracy: 0.5625 - val_loss: 1.9610 - val_accuracy: 0.6676\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.8670 - accuracy: 0.6345 - val_loss: 1.7413 - val_accuracy: 0.6873\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 1.6285 - accuracy: 0.6793 - val_loss: 1.4847 - val_accuracy: 0.7019\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.3800 - accuracy: 0.7221 - val_loss: 1.2476 - val_accuracy: 0.7494\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.1702 - accuracy: 0.7598 - val_loss: 1.0640 - val_accuracy: 0.7853\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.0123 - accuracy: 0.7852 - val_loss: 0.9295 - val_accuracy: 0.7985\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.8960 - accuracy: 0.8036 - val_loss: 0.8299 - val_accuracy: 0.8159\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 0.8078 - accuracy: 0.8165 - val_loss: 0.7530 - val_accuracy: 0.8309\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 0.7389 - accuracy: 0.8278 - val_loss: 0.6920 - val_accuracy: 0.8369\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 0.6837 - accuracy: 0.8364 - val_loss: 0.6426 - val_accuracy: 0.8466\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 0.6388 - accuracy: 0.8442 - val_loss: 0.6027 - val_accuracy: 0.8523\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 0.6017 - accuracy: 0.8498 - val_loss: 0.5685 - val_accuracy: 0.8580\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 0.5706 - accuracy: 0.8560 - val_loss: 0.5399 - val_accuracy: 0.8633\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 0.5443 - accuracy: 0.8605 - val_loss: 0.5156 - val_accuracy: 0.8683\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 0.5218 - accuracy: 0.8654 - val_loss: 0.4957 - val_accuracy: 0.8713\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 0.5023 - accuracy: 0.8686 - val_loss: 0.4771 - val_accuracy: 0.8745\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 0.4852 - accuracy: 0.8722 - val_loss: 0.4610 - val_accuracy: 0.8778\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 0.4701 - accuracy: 0.8748 - val_loss: 0.4473 - val_accuracy: 0.8818\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 0.4568 - accuracy: 0.8778 - val_loss: 0.4347 - val_accuracy: 0.8837\n"
     ]
    }
   ],
   "source": [
    "# before training the NN-model, we need to configure the learning process with the help of (compile method)\n",
    "sigmoid_model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "history_2 = sigmoid_model.fit(X_train,Y_train, steps_per_epoch=500, verbose=1, epochs=nb_epoch,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9400aa4",
   "metadata": {},
   "source": [
    "# MLP + SigmoidActivation + Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79b0833c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 468,874\n",
      "Trainable params: 468,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sigmoid_model = Sequential()\n",
    "sigmoid_model.add(Dense(512, activation='sigmoid', input_shape=(input_dim,)))\n",
    "sigmoid_model.add(Dense(128, activation='sigmoid'))\n",
    "# output_dim represnt number of node on softmax layer\n",
    "# here number of node is 10\n",
    "sigmoid_model.add(Dense(output_dim,activation='softmax'))\n",
    "sigmoid_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91efd78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 7s 13ms/step - loss: 0.5234 - accuracy: 0.8639 - val_loss: 0.2494 - val_accuracy: 0.9278\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.2212 - accuracy: 0.9348 - val_loss: 0.1884 - val_accuracy: 0.9435\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1634 - accuracy: 0.9526 - val_loss: 0.1458 - val_accuracy: 0.9561\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.1248 - accuracy: 0.9632 - val_loss: 0.1288 - val_accuracy: 0.9612\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0981 - accuracy: 0.9706 - val_loss: 0.1085 - val_accuracy: 0.9672\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0786 - accuracy: 0.9767 - val_loss: 0.0910 - val_accuracy: 0.9718\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.0630 - accuracy: 0.9811 - val_loss: 0.0844 - val_accuracy: 0.9741\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0513 - accuracy: 0.9847 - val_loss: 0.0727 - val_accuracy: 0.9777\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0413 - accuracy: 0.9879 - val_loss: 0.0766 - val_accuracy: 0.9757\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0333 - accuracy: 0.9905 - val_loss: 0.0735 - val_accuracy: 0.9776\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0277 - accuracy: 0.9923 - val_loss: 0.0664 - val_accuracy: 0.9800\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0216 - accuracy: 0.9943 - val_loss: 0.0659 - val_accuracy: 0.9808\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0168 - accuracy: 0.9958 - val_loss: 0.0623 - val_accuracy: 0.9808\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0131 - accuracy: 0.9970 - val_loss: 0.0639 - val_accuracy: 0.9807\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0109 - accuracy: 0.9976 - val_loss: 0.0645 - val_accuracy: 0.9815\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0086 - accuracy: 0.9984 - val_loss: 0.0700 - val_accuracy: 0.9796\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0073 - accuracy: 0.9983 - val_loss: 0.0658 - val_accuracy: 0.9817\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0063 - accuracy: 0.9986 - val_loss: 0.0643 - val_accuracy: 0.9826\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.0054 - accuracy: 0.9988 - val_loss: 0.0674 - val_accuracy: 0.9816\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.0881 - val_accuracy: 0.9783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x160814652e0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before traning our NN-model, we need to configure learning process with the help of  (compile method)\n",
    "sigmoid_model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# after configure learning process ,,NN-model train\n",
    "sigmoid_model.fit(X_train,Y_train,batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test,Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd94844",
   "metadata": {},
   "source": [
    "# MLP + ReLu + adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c7db15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_15 (Dense)            (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 468,874\n",
      "Trainable params: 468,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ReLu_model = Sequential()\n",
    "ReLu_model.add(Dense(512, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\n",
    "ReLu_model.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)))\n",
    "# output_dim represnt number of node on softmax layer\n",
    "# here number of node is 10\n",
    "ReLu_model.add(Dense(output_dim,activation='softmax'))\n",
    "ReLu_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc3b4769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 7s 13ms/step - loss: 0.2294 - accuracy: 0.9317 - val_loss: 0.1128 - val_accuracy: 0.9633\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0844 - accuracy: 0.9739 - val_loss: 0.0866 - val_accuracy: 0.9740\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0519 - accuracy: 0.9844 - val_loss: 0.0830 - val_accuracy: 0.9748\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0353 - accuracy: 0.9893 - val_loss: 0.0683 - val_accuracy: 0.9770\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0248 - accuracy: 0.9921 - val_loss: 0.0814 - val_accuracy: 0.9779\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0210 - accuracy: 0.9930 - val_loss: 0.0728 - val_accuracy: 0.9787\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0157 - accuracy: 0.9949 - val_loss: 0.0908 - val_accuracy: 0.9752\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0160 - accuracy: 0.9946 - val_loss: 0.0841 - val_accuracy: 0.9780\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0139 - accuracy: 0.9955 - val_loss: 0.0745 - val_accuracy: 0.9821\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0117 - accuracy: 0.9961 - val_loss: 0.0749 - val_accuracy: 0.9826\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0093 - accuracy: 0.9968 - val_loss: 0.1086 - val_accuracy: 0.9750\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0107 - accuracy: 0.9962 - val_loss: 0.0823 - val_accuracy: 0.9807\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0112 - accuracy: 0.9961 - val_loss: 0.0848 - val_accuracy: 0.9799\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.0861 - val_accuracy: 0.9804\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0079 - accuracy: 0.9974 - val_loss: 0.0946 - val_accuracy: 0.9780\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 0.0989 - val_accuracy: 0.9779\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0055 - accuracy: 0.9982 - val_loss: 0.0871 - val_accuracy: 0.9821\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0070 - accuracy: 0.9976 - val_loss: 0.1121 - val_accuracy: 0.9766\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.0853 - val_accuracy: 0.9834\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0073 - accuracy: 0.9974 - val_loss: 0.0902 - val_accuracy: 0.9823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1609fe9f220>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before traning our NN-model, we need to configure learning process with the help of  (compile method)\n",
    "ReLu_model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# after configure learning process ,,NN-model train\n",
    "ReLu_model.fit(X_train,Y_train,batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test,Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53977557",
   "metadata": {},
   "source": [
    "# MLP + ReLu + sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c19bc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 468,874\n",
      "Trainable params: 468,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ReLu_model = Sequential()\n",
    "ReLu_model.add(Dense(512, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\n",
    "ReLu_model.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)))\n",
    "# output_dim represnt number of node on softmax layer\n",
    "# here number of node is 10\n",
    "ReLu_model.add(Dense(output_dim,activation='softmax'))\n",
    "ReLu_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e7ee29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 5s 9ms/step - loss: 0.7380 - accuracy: 0.7937 - val_loss: 0.3838 - val_accuracy: 0.8921\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.3504 - accuracy: 0.9014 - val_loss: 0.2961 - val_accuracy: 0.9164\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.2888 - accuracy: 0.9179 - val_loss: 0.2566 - val_accuracy: 0.9272\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.2546 - accuracy: 0.9276 - val_loss: 0.2323 - val_accuracy: 0.9326\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.2307 - accuracy: 0.9345 - val_loss: 0.2149 - val_accuracy: 0.9383\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.2126 - accuracy: 0.9397 - val_loss: 0.2004 - val_accuracy: 0.9424\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1975 - accuracy: 0.9442 - val_loss: 0.1902 - val_accuracy: 0.9459\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1852 - accuracy: 0.9480 - val_loss: 0.1810 - val_accuracy: 0.9494\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 4s 7ms/step - loss: 0.1743 - accuracy: 0.9507 - val_loss: 0.1722 - val_accuracy: 0.9512\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.1649 - accuracy: 0.9533 - val_loss: 0.1643 - val_accuracy: 0.9543\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.1567 - accuracy: 0.9561 - val_loss: 0.1574 - val_accuracy: 0.9555\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.1490 - accuracy: 0.9579 - val_loss: 0.1513 - val_accuracy: 0.9564\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1424 - accuracy: 0.9599 - val_loss: 0.1486 - val_accuracy: 0.9580\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.1361 - accuracy: 0.9618 - val_loss: 0.1414 - val_accuracy: 0.9591\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.1305 - accuracy: 0.9636 - val_loss: 0.1376 - val_accuracy: 0.9592\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.1252 - accuracy: 0.9651 - val_loss: 0.1338 - val_accuracy: 0.9604\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.1205 - accuracy: 0.9665 - val_loss: 0.1298 - val_accuracy: 0.9614\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.1160 - accuracy: 0.9679 - val_loss: 0.1261 - val_accuracy: 0.9627\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.1117 - accuracy: 0.9689 - val_loss: 0.1259 - val_accuracy: 0.9629\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.1078 - accuracy: 0.9699 - val_loss: 0.1210 - val_accuracy: 0.9633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x160aeb571f0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before traning our NN-model, we need to configure learning process with the help of  (compile method)\n",
    "ReLu_model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# after configure learning process ,,NN-model train\n",
    "ReLu_model.fit(X_train,Y_train,batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test,Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd07be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
